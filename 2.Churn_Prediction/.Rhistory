ntree=100)
predictions_rf <- predict(rf, newdata = teData)
# Assess model performance
# R-squared
ss_total_rf <- sum((teData$consumption - mean(teData$consumption))^2)
ss_residual_rf <- sum((teData$consumption - predictions_rf)^2)
rsquared_rf <- 1 - (ss_residual_rf / ss_total_rf)
print(paste("R-squared:", rsquared_rf))
# Calculate evaluation metrics
mae_value_rf <- mae(predictions_rf, teData$consumption)
mse_value_rf <- mse(predictions_rf, teData$consumption)
rmse_value_rf <- rmse(predictions_rf, teData$consumption)
# Print the computed metrics
cat("Mean Absolute Error (MAE):", mae_value_rf, "\n")
cat("Mean Squared Error (MSE):", mse_value_rf, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_value_rf, "\n")
library(tidyverse)
library(magrittr)
library(caret)
library(smotefamily)
library(fastDummies)
library(rpart)
library(rpart.plot)
library(ROCR)
library(e1071)
set.seed(1234)
df <- read.csv("churn.csv", header = T, stringsAsFactors = T)
setwd("~/Documents/8.Git_Repos/Portfolio/2.Churn_Prediction")
df <- read.csv("churn.csv", header = T, stringsAsFactors = T)
sum(is.na(df))
prop.table(table(df$churned))
# Create Train vs Test Sets
train_ind <- createDataPartition(df$churned, p = .8,
list = FALSE,
times = 1)
train <- df[train_ind, ]
test <- df[-train_ind, ]
## Resolve Class Imbalance
# Create dummy numericals to feed into SMOTE
train <- dummy_cols(train,
select_columns = c('gender',
'eduLevel',
'marital',
'incomeCat',
'cardCat'))
# Remove original features and apply SMOTE
smote_df <- train[, -c(1,3,5,6,7,8)] %>% SMOTE(train$churned, K = 5)
# Retrieve the data
smote <- smote_df$data
# Rename target label as original
smote <- smote %>% rename(churned = class)
# Check balance
prop.table(table(smote$churned))
sum(is.na(smote))
# Remove ratio features prior to rounding SMOTED observations
smoted <- smote %>% select(-c(delta12amt, delta12count, churned)) %>% round(digits = 0)
# Adding back ratio features
smoted <- cbind(smoted, smote[c('delta12amt', 'delta12count', 'churned')])
#######  Decision Trees  #########
clTree <- rpart(churned~.
,smoted
,method="class"
,control = rpart.control(minsplit = 5))
# Plot cross validation results:
plotcp(clTree)
# Best complexity parameter (cp):
bestcp <- clTree$cptable[which.min(clTree$cptable[,"xerror"]),"CP"]
cp_table <- clTree$cptable
cp_table <- as.data.frame(cp_table)
cp_table %>% ggplot(aes(CP, `xerror`))+geom_line()+
xlab('Complexity Parameter')+
ylab('Cross Validation Error')
# Visualize best CP
bestcp
# Prune and stop the splitting in correspondence of the best cp:
prunedTree <- prune(clTree, cp = bestcp)
#Plot pruned tree:
rpart.plot(prunedTree)
# Feature importance
datfeat <- data.frame(imp = prunedTree$variable.importance)
datfeat2 <- datfeat %>%
tibble::rownames_to_column() %>%
dplyr::rename("variable" = rowname) %>%
dplyr::arrange(imp) %>%
dplyr::mutate(variable = forcats::fct_inorder(variable))
ggplot2::ggplot(datfeat2[1:5,]) +
geom_segment(aes(x = variable, y = 0, xend = variable, yend = imp),
size = .8, alpha = 0.7) +
geom_point(aes(x = variable, y = imp, col = 'blue'),
size = 1.2, show.legend = F) +
coord_flip() +
theme_bw()
# Prepare test data
test <- dummy_cols(test,
select_columns = c('gender',
'eduLevel',
'marital',
'incomeCat',
'cardCat'))
# test  <- test[, -c(1,3,5,6,7,8)]
test$churned <- as.factor(test$churned)
# Make predictions
predictionTree <- prunedTree %>% predict(test, type="class")
recapTree <- confusionMatrix(predictionTree, test$churned)
recapTree
# Check performance
Metric <- recapTree$overall[c(1,2)]
Metric <- as.data.frame(Metric)
Performance_Tree <- recapTree$byClass[c(1,2,5,6,7)]
Performance_Tree <- as.data.frame(Performance_Tree)
Performance_Tree <- Performance_Tree %>% rename(Metric = Performance_Tree)
statsTree <- bind_rows(Metric, Performance_Tree)
View(statsTree)
# Plot ROC:
predTree <- prediction(as.numeric(predictionTree), as.numeric(test$churned))
rocTree <- performance(predTree, "tpr", "fpr")
plot(rocTree, col="red", lwd=3)
abline(a=0, b=1, lwd=3, lty=2)
# Area Under the Curve:
aucTree <- performance(predTree, measure = "auc")
ROC_Tree <- unlist(aucTree@y.values)
ROC_Tree <- as.data.frame(ROC_Tree)
row.names(ROC_Tree)[1] <- "AUC"
ROC_Tree <- ROC_Tree %>% rename(Metric=ROC_Tree)
View(ROC_Tree)
# Performance summary
Tree_summary <- rbind(statsTree, ROC_Tree)
Tree_summary <- Tree_summary %>% rename('Decision Tree Performance' = Metric)
View(Tree_summary)
#########  SVM  #########
# Pre-Compute CV folds to compare kernels
cv_folds <- createMultiFolds(trData, k = 5, times = 5)
# Setting up trControl
trControl <- trainControl(method='repeatedCV',index=cv_folds)
# Fitting models
svmLinear <- train(churned~.,        #-------
train,
method="svmLinear",
trControl=trControl)
View(train)
# Fitting models
svmGrid_l <- expand.grid(.C = c(0.1, 1, 10)
)
svmLinear <- train(x=churned[,2:43],        #-------
y=train[['churned']],
method="svmLinear",
trControl=trControl,
tuneGrid=svmGrid_l)
svmLinear <- train(x=train[,2:43],        #-------
y=train[['churned']],
method="svmLinear",
trControl=trControl,
tuneGrid=svmGrid_l)
svmLinear <- train(
x = train[, predictors],
y = train[[target_column]],
method = "svmLinear",      # SVM with linear kernel
trControl = trControl,
tuneGrid = svmGrid_l
)
target_column <- "curned"
predictors <- setdiff(names(churn), target_column)
# Fitting models
svmGrid_l <- expand.grid(.C = c(0.1, 1, 10)
)
svmLinear <- train(
x = train[, predictors],
y = train[[target_column]],
method = "svmLinear",      # SVM with linear kernel
trControl = trControl,
tuneGrid = svmGrid_l
)
predictors <- setdiff(names(train), target_column)
svmLinear <- train(
x = train[, predictors],
y = train[[target_column]],
method = "svmLinear",      # SVM with linear kernel
trControl = trControl,
tuneGrid = svmGrid_l
)
str(train)
svmLinear <- train(churned~.,
train,
method = "svmLinear",
trControl = trControl,
tuneGrid = svmGrid_l
)
svmLinear <- train(churned~.,
train,
method = "svmLinear",
trControl = trControl)
svmPoly <- train(churned~.,          #--------
train,
method="svmPoly",
trControl=trControl)
View(smoted)
svmLinear <- train(churned~.,
smoted,
method = "svmLinear",
trControl = trControl)
gbmGrid <- expand.grid(
.interaction.depth = c(3, 5, 7),       # Interaction depth
.n.trees = c(100, 200, 300),           # Number of trees
.shrinkage = c(0.01, 0.1, 0.3)         # Shrinkage/learning rate
)
#########  GBM  #########
# Set trControl
trControl <- trainControl(method="cv", number=10)
gbmGrid <- expand.grid(
.interaction.depth = c(3, 5, 7),       # Interaction depth
.n.trees = c(100, 200, 300),           # Number of trees
.shrinkage = c(0.01, 0.1, 0.3)         # Shrinkage/learning rate
)
# Create model
clBoosting <- train(churned~.,
data=smoted,
trControl=trControl,
method='gbm',
verbose=F,
tuneGrid=gbmGrid
)
gbmGrid <- expand.grid(
interaction.depth = c(3, 5, 7),       # Interaction depth
n.trees = c(100, 200, 300),           # Number of trees
shrinkage = c(0.01, 0.1, 0.3)         # Shrinkage/learning rate
)
# Create model
clBoosting <- train(churned~.,
data=smoted,
trControl=trControl,
method='gbm',
verbose=F,
tuneGrid=gbmGrid
)
gbmGrid <- expand.grid(
interaction.depth = c(3, 5, 7),       # Interaction depth
n.trees = c(100, 200, 300),           # Number of trees
shrinkage = c(0.01, 0.1, 0.3),        # Shrinkage/learning rate
n.minobsinnode = c(5, 10, 15)         # Min. Terminal Node Size
)
# Create model
clBoosting <- train(churned~.,
data=smoted,
trControl=trControl,
method='gbm',
verbose=F,
tuneGrid=gbmGrid
)
# Retrieve the best tuning parameters
best_params <- clBoosting$bestTune
print(best_params)
# Make predictions
predBoost <- predict(clBoosting, test)
recapBoost <- confusionMatrix(predBoost, test$churned, positive = '1')
recapBoost
# Get metrics
Metric <- recapBoost$overall[c(1,2)]
recapBoost <- confusionMatrix(predBoost, test$churned)
recapBoost
# Get metrics
Metric <- recapBoost$overall[c(1,2)]
Metric <- as.data.frame(Metric)
Performance_gbm <- recapBoost$byClass[c(1,2,5,6,7)]
Performance_gbm <- as.data.frame(Performance_gbm)
Performance_gbm <- Performance_gbm %>% rename(Metric = Performance_gbm)
statsGbm <- bind_rows(Metric, Performance_gbm)
View(statsGbm)
#ROC:
predGbm <- prediction(as.numeric(predBoost), as.numeric(test$churned))
roc_gbm <- performance(predGbm, "tpr", "fpr")
plot(roc_gbm, col="red", lwd=3)
abline(a=0, b=1, lwd=3, lty=2)
#Area Under the Curve:
auc_gbm <- performance(predGbm, measure = "auc")
ROC_gbm <- unlist(auc_gbm@y.values)
ROC_gbm <- as.data.frame(ROC_gbm)
row.names(ROC_gbm)[1] <- "AUC"
ROC_gbm <- ROC_gbm %>% rename(Metric=ROC_gbm)
View(ROC_gbm)
# Create and view summary
gbm_summary <- rbind(statsGbm, ROC_gbm)
gbm_summary <- gbm_summary %>% rename('RF Performance' = Metric)
View(gbm_summary)
# Plot first 5 most important features
gbmImp <- varImp(clBoosting)
plot(gbmImp, top = 5)
# Plot first 5 most important features
feature_importance <- varImp(clBoosting, scale = FALSE)
# Plot first 5 most important features
feature_importance <- summary(clBoosting)
# Plot first 5 most important features
top_features <- importance(clBoosting, n.trees = 300, scale = FALSE)
# Plot first 5 most important features
feature_importance <- varImp(clBoosting)
# Plot first 5 most important features
underlying_gbm <- clBoosting$finalModel  # Extract the GBM model from the train object
# Get feature importance using the gbm package's summary function
feature_importance <- summary(underlying_gbm)
# Extract the top 5 most important features
top_features <- rownames(feature_importance$importance)[1:5]
print(top_features)
View(feature_importance)
# Extract the top 5 most important features
top_features <- rownames(feature_importance[,c('var', 'rel.Inf')])[1:5]
# Extract the top 5 most important features
top_features <- rownames(feature_importance[,1:2])[1:5]
print(top_features)
feature_importance[,1:2][1:5]
feature_importance[,1:2]
feature_importance[1:5,1:2]
rownames(feature_importance[1:5,1:2])
summary(underlying_gbm)
summary(underlying_gbm)[1:5,1:2]
# Get feature importance using the gbm package's summary function
feature_importance <- summary(underlying_gbm)[1:5,1:2]
View(feature_importance)
# Extract the top 5 most important features
print(feature_importance)
summary(results_ntree)
library(tidyverse)
library(magrittr)
library(caret)
library(smotefamily)
library(fastDummies)
library(rpart)
library(rpart.plot)
library(ROCR)
library(e1071)
set.seed(1234)
df <- read.csv("churn.csv", header = T, stringsAsFactors = T)
sum(is.na(df))
# Create Train vs Test Sets
train_ind <- createDataPartition(df$churned, p = .8,
list = FALSE,
times = 1)
train <- df[train_ind, ]
test <- df[-train_ind, ]
## Resolve Class Imbalance
# Create dummy numerical to feed into SMOTE
train <- dummy_cols(train,
select_columns = c('gender',
'eduLevel',
'marital',
'incomeCat',
'cardCat'))
# Remove original features and apply SMOTE
smote_df <- train[, -c(1,3,5,6,7,8)] %>% SMOTE(train$churned, K = 5)
# Retrieve the data
smote <- smote_df$data
# Rename target label as original
smote <- smote %>% rename(churned = class)
# Remove ratio features prior to rounding SMOTED observations
smoted <- smote %>% select(-c(delta12amt, delta12count, churned)) %>% round(digits = 0)
# Adding back ratio features
smoted <- cbind(smoted, smote[c('delta12amt', 'delta12count', 'churned')])
#######  Decision Trees  #########
clTree <- rpart(churned~.
,smoted
,method="class"
,control = rpart.control(minsplit = 5))
# Plot cross validation results:
plotcp(clTree)
# Best complexity parameter (cp):
bestcp <- clTree$cptable[which.min(clTree$cptable[,"xerror"]),"CP"]
cp_table <- clTree$cptable
cp_table <- as.data.frame(cp_table)
cp_table %>% ggplot(aes(CP, `xerror`))+geom_line()+
xlab('Complexity Parameter')+
ylab('Cross Validation Error')
# Visualize best CP
bestcp
# Prune and stop the splitting in correspondence of the best cp:
prunedTree <- prune(clTree, cp = bestcp)
#Plot pruned tree:
rpart.plot(prunedTree)
# Feature importance
datfeat <- data.frame(imp = prunedTree$variable.importance)
datfeat2 <- datfeat %>%
tibble::rownames_to_column() %>%
dplyr::rename("variable" = rowname) %>%
dplyr::arrange(imp) %>%
dplyr::mutate(variable = forcats::fct_inorder(variable))
# Plot first 5 most important features
ggplot2::ggplot(datfeat2[1:5,]) +
geom_segment(aes(x = variable, y = 0, xend = variable, yend = imp),
size = .8, alpha = 0.7) +
geom_point(aes(x = variable, y = imp, col = 'blue'),
size = 1.2, show.legend = F) +
coord_flip() +
theme_bw()
# Prepare test data
test <- dummy_cols(test,
select_columns = c('gender',
'eduLevel',
'marital',
'incomeCat',
'cardCat'))
# test  <- test[, -c(1,3,5,6,7,8)]
test$churned <- as.factor(test$churned)
# Make predictions
predictionTree <- prunedTree %>% predict(test, type="class")
recapTree <- confusionMatrix(predictionTree, test$churned)
recapTree
# Check performance
Metric <- recapTree$overall[c(1,2)]
Metric <- as.data.frame(Metric)
Performance_Tree <- recapTree$byClass[c(1,2,5,6,7)]
Performance_Tree <- as.data.frame(Performance_Tree)
Performance_Tree <- Performance_Tree %>% rename(Metric = Performance_Tree)
statsTree <- bind_rows(Metric, Performance_Tree)
View(statsTree)
# Plot ROC:
predTree <- prediction(as.numeric(predictionTree), as.numeric(test$churned))
rocTree <- performance(predTree, "tpr", "fpr")
plot(rocTree, col="red", lwd=3)
abline(a=0, b=1, lwd=3, lty=2)
# Area Under the Curve:
aucTree <- performance(predTree, measure = "auc")
ROC_Tree <- unlist(aucTree@y.values)
ROC_Tree <- as.data.frame(ROC_Tree)
row.names(ROC_Tree)[1] <- "AUC"
ROC_Tree <- ROC_Tree %>% rename(Metric=ROC_Tree)
View(ROC_Tree)
# Performance summary
Tree_summary <- rbind(statsTree, ROC_Tree)
Tree_summary <- Tree_summary %>% rename('Decision Tree Performance' = Metric)
View(Tree_summary)
#########  GBM  #########
# Set trControl
trControl <- trainControl(method="cv", number=10)
gbmGrid <- expand.grid(
interaction.depth = c(3, 5, 7),
n.trees = c(100, 200, 300),
shrinkage = c(0.01, 0.1, 0.3),
n.minobsinnode = c(5, 10, 15)
)
# Create model
clBoosting <- train(churned~.,
data=smoted,
trControl=trControl,
method='gbm',
verbose=F,
tuneGrid=gbmGrid
)
# Retrieve the best tuning parameters
best_params <- clBoosting$bestTune
print(best_params)
plot(clBoosting)
# Make predictions
predBoost <- predict(clBoosting, test)
recapBoost <- confusionMatrix(predBoost, test$churned)
recapBoost
# Get metrics
Metric <- recapBoost$overall[c(1,2)]
Metric <- as.data.frame(Metric)
Performance_gbm <- recapBoost$byClass[c(1,2,5,6,7)]
Performance_gbm <- as.data.frame(Performance_gbm)
Performance_gbm <- Performance_gbm %>% rename(Metric = Performance_gbm)
statsGbm <- bind_rows(Metric, Performance_gbm)
View(statsGbm)
#ROC:
predGbm <- prediction(as.numeric(predBoost), as.numeric(test$churned))
roc_gbm <- performance(predGbm, "tpr", "fpr")
plot(roc_gbm, col="red", lwd=3)
abline(a=0, b=1, lwd=3, lty=2)
#Area Under the Curve:
auc_gbm <- performance(predGbm, measure = "auc")
ROC_gbm <- unlist(auc_gbm@y.values)
ROC_gbm <- as.data.frame(ROC_gbm)
row.names(ROC_gbm)[1] <- "AUC"
ROC_gbm <- ROC_gbm %>% rename(Metric=ROC_gbm)
View(ROC_gbm)
# Create and view summary
gbm_summary <- rbind(statsGbm, ROC_gbm)
gbm_summary <- gbm_summary %>% rename('RF Performance' = Metric)
View(gbm_summary)
# Plot first 5 most important features
underlying_gbm <- clBoosting$finalModel  # Extract the GBM model from the train object
# Get feature importance using the gbm package's summary function
feature_importance <- summary(underlying_gbm)[1:5,1:2]
# Extract the top 5 most important features
print(feature_importance)
# Get feature importance using the gbm package's summary function
feature_importance <- summary(underlying_gbm)[1:5,1:2]
View(datfeat2)
View(feature_importance)
ggplot2::ggplot(feature_importance[1:5,]) +
geom_segment(aes(x = variable, y = 0, xend = variable, yend = imp),
size = .8, alpha = 0.7) +
geom_point(aes(x = variable, y = imp, col = 'blue'),
size = 1.2, show.legend = F) +
coord_flip() +
theme_bw()
ggplot2::ggplot(feature_importance[1:5,]) +
geom_segment(aes(x = var, y = 0, xend = var, yend = rel.Inf),
size = .8, alpha = 0.7) +
geom_point(aes(x = var, y = rel.Inf, col = 'blue'),
size = 1.2, show.legend = F) +
coord_flip() +
theme_bw()
ggplot2::ggplot(feature_importance[1:5,]) +
geom_segment(aes(x = var, y = 0, xend = var, yend = rel.inf),
size = .8, alpha = 0.7) +
geom_point(aes(x = var, y = rel.inf, col = 'blue'),
size = 1.2, show.legend = F) +
coord_flip() +
theme_bw()
ggplot2::ggplot(feature_importance[1:5,]) +
geom_segment(aes(x = var, y = 0, xend = var, yend = rel.inf), size = .8, alpha = 0.7) +
geom_point(aes(x = var, y = rel.inf, col = 'blue'), size = 1.2, show.legend = F) +
coord_flip() +
theme_bw()
